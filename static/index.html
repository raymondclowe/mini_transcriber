<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Mini Transcriber Live</title>
  <style>
    body { font-family: system-ui, Arial; margin: 24px; }
    #log { white-space: pre-wrap; border: 1px solid #ddd; padding: 12px; height: 200px; overflow: auto }
    button { padding: 8px 12px; font-size: 16px }
  </style>
</head>
<body>
  <h1>Mini Transcriber (live chunks)</h1>
  <p>Press Start to capture microphone audio in overlapping chunks (5s segments, 1s overlap).
  Each chunk is POSTed to /transcribe and results are appended to the transcript area with
  a simple duplicate-trim heuristic.</p>

  <button id="btn">Start</button>
  <div style="margin-top:12px;">
    <strong>Transcript:</strong>
    <div id="log"></div>
  </div>

<script>
(async function(){
  const btn = document.getElementById('btn');
  const log = document.getElementById('log');
  let running = false;
  let mediaStream = null;
  let recorder = null;
  let audioCtx = null;
  let source = null;
  let overlapMs = 1000; // 1s overlap
  let segmentMs = 5000; // 5s segments
  let seq = 0;
  let lastText = '';

  function appendText(t){
    // naive dedupe: if lastText endsWith start of t, remove overlap
    let out = t;
    if(lastText && t){
      // trim whitespace
      const a = lastText.trim();
      const b = t.trim();
      // If new text starts with the tail of previous, drop that prefix
      for(let cut=Math.min(a.length, b.length); cut>3; cut--){
        if(b.startsWith(a.slice(-cut))){
          out = b.slice(cut).trimStart();
          break;
        }
      }
    }
    if(out){
      lastText = (lastText + ' ' + out).trim();
      log.textContent = lastText;
    }
  }

  async function postChunk(blob){
    try{
      const fd = new FormData();
      fd.append('file', blob, `chunk-${seq++}.wav`);
      const r = await fetch('/transcribe', { method: 'POST', body: fd });
      if(!r.ok){ console.warn('transcribe failed', r.status); return }
      const j = await r.json();
      appendText(j.text || '');
    }catch(e){ console.error(e) }
  }

  function startRecording(){
    running = true; btn.textContent='Stop'; lastText=''; log.textContent=''; seq=0;
    navigator.mediaDevices.getUserMedia({ audio: true }).then(stream=>{
      mediaStream = stream;
      audioCtx = new AudioContext({sampleRate: 16000});
      source = audioCtx.createMediaStreamSource(stream);
      const dest = audioCtx.createMediaStreamDestination();
      source.connect(dest);
      // Use MediaRecorder from the dest stream; ensure codec is wav via mimeType fallback
      const rec = new MediaRecorder(dest.stream);
      let chunks = [];
      let startTime = Date.now();

      rec.ondataavailable = async (e) => {
        if(e.data && e.data.size>0) chunks.push(e.data);
      };

      // We will capture overlapping chunks by stopping/starting and keeping last overlap
      // Start an interval to slice recorded data into windows
      rec.start();
      recorder = rec;

      let sliding = [];
  const interval = setInterval(async ()=>{
        // extract audio for the last segmentMs milliseconds by requesting a slice
        // Stop the recorder briefly to get a blob, then restart and keep overlap
        if(recorder.state === 'inactive') return;
        recorder.requestData();
        // wait a tick for ondataavailable to fire
        await new Promise(r=>setTimeout(r, 200));
        // combine available chunks into a blob
        const blob = new Blob(chunks, { type: 'audio/webm' });
        chunks = [];
        try{
          // decode to audio buffer, resample to 8kHz, then convert to WAV PCM16
          const array = await blob.arrayBuffer();
          const decoded = await audioCtx.decodeAudioData(array);
          // create an offline context at 8kHz to render the buffer
          const offline = new OfflineAudioContext(1, Math.ceil(decoded.duration * 8000), 8000);
          const src = offline.createBufferSource();
          // copy channel data (mix to mono if needed)
          const mono = offline.createBuffer(1, decoded.length, decoded.sampleRate);
          const idata = decoded.getChannelData(0);
          mono.getChannelData(0).set(idata);
          src.buffer = mono;
          src.connect(offline.destination);
          src.start(0);
          const rendered = await offline.startRendering();
          // convert rendered buffer to WAV PCM16
          const wav = audioBufferToWav(rendered, 8000);
          const wavBlob = new Blob([wav], { type: 'audio/wav' });
          postChunk(wavBlob);
        }catch(e){
          console.warn('resample failed, sending raw blob', e);
          postChunk(blob);
        }
        // for overlap: do nothing special â€” the browser capture is continuous; requestData gives us last slice
      }, segmentMs - overlapMs);

      // save the interval ID so we can clear it on stop
      rec._interval = interval;

    }).catch(err=>{ console.error(err); running=false; btn.textContent='Start' });
  }

  // convert AudioBuffer to 16-bit PCM WAV (Uint8Array)
  function audioBufferToWav(buffer, sampleRate) {
    const numChannels = buffer.numberOfChannels;
    const length = buffer.length * numChannels * 2 + 44;
    const bufferArray = new ArrayBuffer(length);
    const view = new DataView(bufferArray);

    /* RIFF identifier */ writeString(view, 0, 'RIFF');
    /* file length */ view.setUint32(4, 36 + buffer.length * numChannels * 2, true);
    /* RIFF type */ writeString(view, 8, 'WAVE');
    /* format chunk identifier */ writeString(view, 12, 'fmt ');
    /* format chunk length */ view.setUint32(16, 16, true);
    /* sample format (raw) */ view.setUint16(20, 1, true);
    /* channel count */ view.setUint16(22, numChannels, true);
    /* sample rate */ view.setUint32(24, sampleRate, true);
    /* byte rate (sample rate * block align) */ view.setUint32(28, sampleRate * numChannels * 2, true);
    /* block align (channel count * bytes per sample) */ view.setUint16(32, numChannels * 2, true);
    /* bits per sample */ view.setUint16(34, 16, true);
    /* data chunk identifier */ writeString(view, 36, 'data');
    /* data chunk length */ view.setUint32(40, buffer.length * numChannels * 2, true);

    // write interleaved PCM16
    let offset = 44;
    for (let i = 0; i < buffer.length; i++){
      for (let ch = 0; ch < numChannels; ch++){
        let sample = buffer.getChannelData(ch)[i];
        // clamp
        sample = Math.max(-1, Math.min(1, sample));
        view.setInt16(offset, sample < 0 ? sample * 0x8000 : sample * 0x7FFF, true);
        offset += 2;
      }
    }

    return bufferArray;
  }

  function writeString(view, offset, string){
    for(let i=0;i<string.length;i++) view.setUint8(offset+i, string.charCodeAt(i));
  }

  function stopRecording(){
    running=false; btn.textContent='Start';
    if(recorder){
      clearInterval(recorder._interval);
      recorder.stop(); recorder = null;
    }
    if(mediaStream){
      mediaStream.getTracks().forEach(t=>t.stop()); mediaStream=null;
    }
    if(audioCtx){ audioCtx.close(); audioCtx=null }
  }

  // feature-detect getUserMedia and provide a helpful message if missing
  const canGetUserMedia = !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia);
  if(!canGetUserMedia){
    btn.disabled = true;
    const msg = document.createElement('div');
    msg.style.marginTop = '8px';
    msg.style.color = '#b00';
    msg.textContent = 'Microphone capture is not available in this browser. Try a recent Chrome, Edge, or Firefox and ensure the page is served over https or localhost.';
    btn.parentNode.insertBefore(msg, btn.nextSibling);
  } else {
    btn.addEventListener('click', ()=>{
      if(!running) startRecording(); else stopRecording();
    });
  }
})();
</script>
</body>
</html>
